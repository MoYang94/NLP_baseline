{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备预料\n",
    "corpus = ['he is a king',\n",
    "          'she is a queen',\n",
    "          'he is a man',\n",
    "          'she is a woman',\n",
    "          'warsaw is poland capital',\n",
    "          'berlin is germany capital',\n",
    "          'paris is france capital']\n",
    "corpus_list = [sentence.split() for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['warsaw', 'is', 'poland', 'capital'], ['berlin', 'is', 'germany', 'capital'], ['paris', 'is', 'france', 'capital']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 0, 'is': 1, 'a': 2, 'king': 3, 'she': 4, 'queen': 5, 'man': 6, 'woman': 7, 'warsaw': 8, 'poland': 9, 'capital': 10, 'berlin': 11, 'germany': 12, 'paris': 13, 'france': 14}\n"
     ]
    }
   ],
   "source": [
    "#构建词典\n",
    "word2ix = {}\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split():\n",
    "        if word not in word2ix:\n",
    "            word2ix[word] = len(word2ix) #为每个词匹配一个索引index\n",
    "print(word2ix)\n",
    "        \n",
    "ix2word = {v:k for k,v in word2ix.items()}#将dict中的key与value互换位置\n",
    "voc_size = len(word2ix)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "center_word_index: 0\n",
      "center_word_ix: 0\n",
      "is\n",
      "center_word_index: 1\n",
      "center_word_ix: 1\n",
      "a\n",
      "center_word_index: 2\n",
      "center_word_ix: 2\n",
      "king\n",
      "center_word_index: 3\n",
      "center_word_ix: 3\n",
      "she\n",
      "center_word_index: 0\n",
      "center_word_ix: 4\n",
      "is\n",
      "center_word_index: 1\n",
      "center_word_ix: 1\n",
      "a\n",
      "center_word_index: 2\n",
      "center_word_ix: 2\n",
      "queen\n",
      "center_word_index: 3\n",
      "center_word_ix: 5\n",
      "he\n",
      "center_word_index: 0\n",
      "center_word_ix: 0\n",
      "is\n",
      "center_word_index: 1\n",
      "center_word_ix: 1\n",
      "a\n",
      "center_word_index: 2\n",
      "center_word_ix: 2\n",
      "man\n",
      "center_word_index: 3\n",
      "center_word_ix: 6\n",
      "she\n",
      "center_word_index: 0\n",
      "center_word_ix: 4\n",
      "is\n",
      "center_word_index: 1\n",
      "center_word_ix: 1\n",
      "a\n",
      "center_word_index: 2\n",
      "center_word_ix: 2\n",
      "woman\n",
      "center_word_index: 3\n",
      "center_word_ix: 7\n",
      "warsaw\n",
      "center_word_index: 0\n",
      "center_word_ix: 8\n",
      "is\n",
      "center_word_index: 1\n",
      "center_word_ix: 1\n",
      "poland\n",
      "center_word_index: 2\n",
      "center_word_ix: 9\n",
      "capital\n",
      "center_word_index: 3\n",
      "center_word_ix: 10\n",
      "berlin\n",
      "center_word_index: 0\n",
      "center_word_ix: 11\n",
      "is\n",
      "center_word_index: 1\n",
      "center_word_ix: 1\n",
      "germany\n",
      "center_word_index: 2\n",
      "center_word_ix: 12\n",
      "capital\n",
      "center_word_index: 3\n",
      "center_word_ix: 10\n",
      "paris\n",
      "center_word_index: 0\n",
      "center_word_ix: 13\n",
      "is\n",
      "center_word_index: 1\n",
      "center_word_ix: 1\n",
      "france\n",
      "center_word_index: 2\n",
      "center_word_ix: 14\n",
      "capital\n",
      "center_word_index: 3\n",
      "center_word_ix: 10\n",
      "[(0, 1), (0, 2), (1, 0), (1, 2), (1, 3), (2, 0), (2, 1), (2, 3), (3, 1), (3, 2), (4, 1), (4, 2), (1, 4), (1, 2), (1, 5), (2, 4), (2, 1), (2, 5), (5, 1), (5, 2), (0, 1), (0, 2), (1, 0), (1, 2), (1, 6), (2, 0), (2, 1), (2, 6), (6, 1), (6, 2), (4, 1), (4, 2), (1, 4), (1, 2), (1, 7), (2, 4), (2, 1), (2, 7), (7, 1), (7, 2), (8, 1), (8, 9), (1, 8), (1, 9), (1, 10), (9, 8), (9, 1), (9, 10), (10, 1), (10, 9), (11, 1), (11, 12), (1, 11), (1, 12), (1, 10), (12, 11), (12, 1), (12, 10), (10, 1), (10, 12), (13, 1), (13, 14), (1, 13), (1, 14), (1, 10), (14, 13), (14, 1), (14, 10), (10, 1), (10, 14)]\n"
     ]
    }
   ],
   "source": [
    "#构建训练对\n",
    "WINDOWS = 2 # 取左右窗口的词作为context_word\n",
    "pairs = [] # 存放训练对\n",
    "\n",
    "for sentence in corpus_list:\n",
    "    for center_word_index in range(len(sentence)):\n",
    "        center_word_ix = word2ix[sentence[center_word_index]]\n",
    "        print(sentence[center_word_index])\n",
    "        print('center_word_index:',center_word_index)\n",
    "        print('center_word_ix:',center_word_ix)\n",
    "        for win in range(-WINDOWS,WINDOWS+1):\n",
    "            contenx_word_index = center_word_index + win\n",
    "            if 0<=contenx_word_index<=len(sentence)-1 and contenx_word_index != center_word_index:\n",
    "                contenx_word_ix = word2ix[sentence[contenx_word_index]]\n",
    "                pairs.append((center_word_ix,contenx_word_ix))\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, voc_dim, emb_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        # 初始化参数\n",
    "        self.embedding_matrix = nn.Parameter(torch.FloatTensor(emb_dim, voc_dim))\n",
    "        self.W = nn.Parameter(torch.FloatTensor(voc_dim, emb_dim))\n",
    "        torch.nn.init.xavier_normal(self.embedding_matrix)\n",
    "        torch.nn.init.xavier_normal(self.W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = torch.matmul(self.embedding_matrix, x)\n",
    "        h = torch.matmul(self.W, emb)  # [voc_dim]\n",
    "        log_softmax = F. log_softmax(h)  # [voc_dim]\n",
    "\n",
    "        return log_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15896\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  import sys\n",
      "C:\\Users\\15896\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  \n",
      "C:\\Users\\15896\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 193.154262\n",
      "epoch: 2, loss: 178.086491\n",
      "epoch: 4, loss: 156.701561\n",
      "epoch: 6, loss: 141.661043\n",
      "epoch: 8, loss: 133.716635\n"
     ]
    }
   ],
   "source": [
    "# 提前设置超参数\n",
    "epoch = 10\n",
    "lr = 1e-2\n",
    "embedding_dim = 5\n",
    "\n",
    "# 模型、优化器、损失\n",
    "model = SkipGram(voc_size, embedding_dim)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_f = torch.nn.NLLLoss()  \n",
    "\n",
    "# 这是将索引变成词典大小的One-Hot向量的方法\n",
    "def get_onehot_vector(ix):\n",
    "    one_hot_vec = torch.zeros(voc_size).float()\n",
    "    one_hot_vec[ix] = 1.0\n",
    "    return one_hot_vec\n",
    "\n",
    "# 迭代\n",
    "for e in range(epoch):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (center_ix, context_ix) in enumerate(pairs):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # 预处理好数据结构\n",
    "        one_hot_vec = get_onehot_vector(center_ix)\n",
    "        y_true = torch.Tensor([context_ix]).long()\n",
    "\n",
    "        # 前向\n",
    "        y_pred = model(one_hot_vec)\n",
    "        loss = loss_f(y_pred.view(1, -1), y_true)\n",
    "\n",
    "        # 后向\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.data.item()\n",
    "\n",
    "        # 梯度更新\n",
    "        optim.step()\n",
    "\n",
    "    if e % 2 == 0:\n",
    "        print('epoch: %d, loss: %f' % (e, epoch_loss))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5483,  0.7743, -0.1095,  1.4099, -0.0963], grad_fn=<MvBackward>)\n",
      "tensor([-0.8205,  0.5341,  0.0617,  1.7159, -0.2900], grad_fn=<MvBackward>)\n",
      "tensor([ 0.1909, -0.1515,  1.2479,  0.3793,  1.1203], grad_fn=<MvBackward>)\n",
      "tensor(0.9228, grad_fn=<DivBackward0>)\n",
      "tensor(-0.0316, grad_fn=<DivBackward0>)\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n",
      "词：%f,向量：%f\n"
     ]
    }
   ],
   "source": [
    "# # 3.预测：预测单词的向量并计算相似度\n",
    "v1 = torch.matmul(model.embedding_matrix, get_onehot_vector((word2ix['he'])))\n",
    "v2 = torch.matmul(model.embedding_matrix, get_onehot_vector((word2ix['she'])))\n",
    "v3 = torch.matmul(model.embedding_matrix, get_onehot_vector((word2ix['capital'])))\n",
    "\n",
    "print(v1)\n",
    "print(v2)\n",
    "print(v3)\n",
    "\n",
    "s_v1_v2 = F.cosine_similarity(v1, v2, dim=0)\n",
    "s_v1_v3 = F.cosine_similarity(v1, v3, dim=0)\n",
    "print(s_v1_v2)\n",
    "print(s_v1_v3)\n",
    "for i in word2ix:\n",
    "    v = torch.matmul(model.embedding_matrix, get_onehot_vector((word2ix['he'])))\n",
    "    print('词：%f,向量：%f'.format( word2ix[i],v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
